{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcpy\n",
    "import os\n",
    "import numpy\n",
    "import pandas\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load FRED/RTI microdata into database from files.\n",
    "\n",
    "    Assumes a folder called \"data/\" with many subfolders representing counties. Each county subfolder should have a file \"households.txt\". Concatenates all these county incomes together and saves the full list as a csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arcpy.env.workspace = \"C:\\\\Users\\\\Jayant\\\\Desktop\\\\SSTD\\\\workspace\"\n",
    "print(arcpy.env.workspace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_households_file(data_root_folder, save_filename):\n",
    "    full_dataset = pandas.DataFrame()  # empty dataframe to start\n",
    "    numrecs = 0\n",
    "    #directory = os.fsencode(data_root_folder)\n",
    "    data_root_folder = os.path.join(arcpy.env.workspace, data_root_folder)\n",
    "    for county_folder in os.listdir(data_root_folder):\n",
    "        county_foldername = os.fsdecode(county_folder)\n",
    "        folderpath = os.fsdecode(os.path.join(data_root_folder,county_foldername))\n",
    "        if (folderpath.endswith(('.zip', '.csv'))):   # ignore anything that isn't a folder\n",
    "            continue        \n",
    "        for file in os.listdir(folderpath):\n",
    "            filename = os.fsdecode(file)\n",
    "            if (filename == \"households.txt\"):     # read households file\n",
    "                filepath = os.fsdecode(os.path.join(folderpath,filename))\n",
    "                print(\"Loading file: {0}\".format(filepath))\n",
    "                file_data = pandas.read_csv(filepath_or_buffer = filepath,\n",
    "                                            sep='\\t',                      # tab-delimited\n",
    "                                            header=0,\n",
    "                                            index_col='sp_id',\n",
    "                                            usecols=['sp_id', 'hh_race', 'hh_income', 'latitude', 'longitude'],\n",
    "                                            dtype={\n",
    "                                                'sp_id': str,\n",
    "                                                'hh_race': int,\n",
    "                                                'hh_income': int,\n",
    "                                                'latitude': str,\n",
    "                                                'longitude': str\n",
    "                                            },\n",
    "                                            engine='c' )\n",
    "                (nrows, _) = file_data.shape\n",
    "                print(\"Read {0} income records. Merging with dataset\".format(nrows))\n",
    "                numrecs += nrows\n",
    "                full_dataset = pandas.concat([full_dataset, file_data])\n",
    "                \n",
    "    # write dataset to file\n",
    "    print(\"\\nFinished merging {0} income records together. Writing full file to disk...\".format(numrecs))\n",
    "    save_fpath = os.fsdecode(os.path.join(data_root_folder, save_filename))\n",
    "    full_dataset.to_csv(path_or_buf=save_fpath)\n",
    "    print(\"Done.\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_join_people_with_households(data_root_folder, save_filename):\n",
    "    full_dataset = pandas.DataFrame()  # empty dataframe to start\n",
    "    numrecs = 0\n",
    "    test_flag = 0\n",
    "    #directory = os.fsencode(data_root_folder)\n",
    "    data_root_folder = os.path.join(arcpy.env.workspace, data_root_folder)\n",
    "    for county_folder in os.listdir(data_root_folder):\n",
    "        county_foldername = os.fsdecode(county_folder)\n",
    "        folderpath = os.fsdecode(os.path.join(data_root_folder,county_foldername))\n",
    "        if (folderpath.endswith(('.zip', '.csv'))):   # ignore anything that isn't a folder\n",
    "            continue        \n",
    "        for file in os.listdir(folderpath):\n",
    "            filename = os.fsdecode(file)\n",
    "            if (filename == \"people.txt\"):     # read people file\n",
    "                filepath = os.fsdecode(os.path.join(folderpath,filename))\n",
    "                print(\"Loading file: {0}\".format(filepath))\n",
    "                people_file_data = pandas.read_csv(filepath_or_buffer = filepath,\n",
    "                                            sep='\\t',                      # tab-delimited\n",
    "                                            header=0,\n",
    "                                            #index_col='sp_id',\n",
    "                                            usecols=['sp_id', 'sp_hh_id'],\n",
    "                                            dtype={\n",
    "                                                'sp_id': str,\n",
    "                                                'sp_hh_id': str,\n",
    "                                            },\n",
    "                                            engine='c' )\n",
    "               \n",
    "                (nrows, _) = people_file_data.shape\n",
    "                print(\"Read {0} people records. Aggregating to get the house size\".format(nrows))\n",
    "                hh_size = people_file_data.groupby('sp_hh_id', as_index=False).count()\n",
    "                \n",
    "                #hh_size.set_index('sp_hh_id')\n",
    "            if(filename == \"households.txt\"): # read households file\n",
    "                filepath = os.fsdecode(os.path.join(folderpath,filename))\n",
    "                print(\"Loading file: {0}\".format(filepath))\n",
    "                hh_file_data = pandas.read_csv(filepath_or_buffer = filepath,\n",
    "                                            sep='\\t',                      # tab-delimited\n",
    "                                            header=0,\n",
    "                          #                  index_col='sp_id',\n",
    "                                            usecols=['sp_id', 'hh_race', 'hh_income', 'latitude', 'longitude'],\n",
    "                                            dtype={\n",
    "                                                'sp_id': str,\n",
    "                                                'hh_race': int,\n",
    "                                                'hh_income': int,\n",
    "                                                'latitude': str,\n",
    "                                                'longitude': str\n",
    "                                            },\n",
    "                                            engine='c' )\n",
    "                (nrows, _) = hh_file_data.shape\n",
    "                print(\"Read {0} income records. joining with household size\".format(nrows))\n",
    "#        \n",
    "        hh_file_data = hh_file_data.rename(columns={'sp_id':'sp_hh_id'})\n",
    "        hh_size = hh_size.rename(columns={'sp_id':'hh_size'})\n",
    "        hh_size.set_index('sp_hh_id')\n",
    "        hh_file_data.set_index('sp_hh_id')\n",
    "        \n",
    "        file_data = hh_file_data.merge(hh_size)\n",
    "        full_dataset = pandas.concat([full_dataset, file_data])\n",
    "#        file_data = hh_file_data.join(hh_size)\n",
    "        (nrows, _) = file_data.shape\n",
    "        \n",
    "        numrecs += nrows\n",
    "        if(test_flag == 1):\n",
    "            break\n",
    "    # write dataset to file\n",
    "    print(\"\\nFinished merging {0} income records together. Writing full file to disk...\".format(numrecs))\n",
    "    save_fpath = os.fsdecode(os.path.join(data_root_folder, save_filename))\n",
    "    full_dataset.to_csv(path_or_buf=save_fpath)\n",
    "    print(\"Done.\")\n",
    "    return\n",
    "\n",
    "\n",
    "# Condense many households files into one dataframe by iterating through data folder\n",
    "data_folder = \"..\\\\data\"            # position of data folder relative to this notebook file\n",
    "save_fname = \"rti_race_incomes.csv\"     # filename to export merged csv as (within data_folder directory)\n",
    " \n",
    "read_join_people_with_households(data_folder, save_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Condense many households files into one dataframe by iterating through data folder\n",
    "data_folder = \"..\\\\data\"            # position of data folder relative to this notebook file\n",
    "save_fname = \"rti_race_incomes.csv\"     # filename to export merged csv as (within data_folder directory)\n",
    " \n",
    "read_households_file(data_folder, save_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geocode table\n",
    "\n",
    "# data_folder = \"../data\"            # position of data folder relative to this notebook file\n",
    "# save_fname = \"rti_incomes.csv\"     # filename to export merged csv as (within data_folder directory)\n",
    "# out_tablename = \"rti_income_table\"\n",
    "\n",
    "# rti_incomes_path = os.fsdecode(os.path.join(data_folder, save_fname))\n",
    "\n",
    "# outtable = arcpy.TableToTable_conversion(in_rows = rti_incomes_path, \n",
    " #                    out_path = arcpy.env.workspace, \n",
    "  #                   out_name = out_tablename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rti_income_feature_class = \"rti_race_income_hhsize_feature_class\"\n",
    "arcpy.management.XYTableToPoint(in_table = \"C:\\\\Users\\\\Jayant\\\\Desktop\\\\SSTD\\\\data\\\\rti_race_incomes.csv\", \n",
    "                       out_feature_class = rti_income_feature_class, \n",
    "                       x_field = \"longitude\", \n",
    "                       y_field = \"latitude\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(arcpy.GetCount_management('rti_joined_feature_class.shp'))\n",
    "print(arcpy.GetCount_management('rti_race_income_feature_class.shp'))\n",
    "print(arcpy.GetCount_management('rti_joined_ri_feature_class.shp'))\n",
    "print(arcpy.GetCount_management('rti_joined_rih_feature_class.shp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computationally efficient Gini function (from https://github.com/oliviaguest/gini) \n",
    "#    and https://www.statsdirect.com/help/default.htm#nonparametric_methods/gini.htm\n",
    "def gini(array):\n",
    "    array = array.flatten()\n",
    "    print(\"Calculating Gini. Example of data: \")\n",
    "    print(array[:5])\n",
    "    # Values cannot be negative:\n",
    "    if (numpy.amin(array) < 0):\n",
    "        array -= numpy.amin(array)\n",
    "    \n",
    "    # Sort values:\n",
    "    array = numpy.sort(array)\n",
    "    \n",
    "    # Index and count of array elements:\n",
    "    index = numpy.arange(1,array.shape[0]+1)\n",
    "    n = array.shape[0]\n",
    "    \n",
    "    # Gini coefficient:\n",
    "    return (   (  numpy.sum(( (2*index) - n - 1 ) * array)  ) / (n * numpy.sum(array))   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incomefile = os.path.join(arcpy.env.workspace, \"..\\\\data\\\\rti_incomes.csv\")\n",
    "incomelist = pandas.read_csv(filepath_or_buffer=incomefile,\n",
    "                             header=0,\n",
    "                             index_col=False,\n",
    "                             usecols=['hh_income'],\n",
    "                             dtype={'hh_income': numpy.longlong},\n",
    "                             engine='c')\n",
    "print(\"Read in file. Calculating Gini...\")\n",
    "g = gini(incomelist.to_numpy())\n",
    "print(\"Gini is {0}\".format(g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arcpy.env.workspace = \"C:\\\\Users\\\\Jayant\\\\Desktop\\\\SSTD\\\\workspace\"\n",
    "fields = [field.name for field in arcpy.ListFields('tl_2020_27_bg20.shp')]\n",
    "print(fields)\n",
    "print(\"\\n\")\n",
    "fields = [field.name for field in arcpy.ListFields('tl_2020_27_tabblock20.shp')]\n",
    "print(fields)\n",
    "\n",
    "print(arcpy.GetCount_management('tl_2020_27_bg20.shp'))\n",
    "print(arcpy.GetCount_management('tl_2020_27_tabblock20.shp'))\n",
    "\n",
    "arcpy.CopyFeatures_management('tl_2020_27_bg20.shp','bg_20_filter.shp')\n",
    "dropFields = ['NAMELSAD20', 'MTFCC20', 'FUNCSTAT20', 'ALAND20', 'AWATER20', 'INTPTLAT20', 'INTPTLON20']\n",
    "arcpy.DeleteField_management('bg_20_filter.shp', dropFields)\n",
    "\n",
    "arcpy.CopyFeatures_management('tl_2020_27_tabblock20.shp','tabblock_20_filter.shp')\n",
    "dropFields = ['STATEFP20', 'COUNTYFP20', 'TRACTCE20', 'NAME20', 'MTFCC20', 'UR20', 'UACE20', 'UATYPE20', 'FUNCSTAT20', 'ALAND20', 'AWATER20', 'INTPTLAT20', 'INTPTLON20']\n",
    "arcpy.DeleteField_management('tabblock_20_filter.shp', dropFields)\n",
    "fields = [field.name for field in arcpy.ListFields('bg_20_filter.shp')]\n",
    "print(fields)\n",
    "fields = [field.name for field in arcpy.ListFields('tabblock_20_filter.shp')]\n",
    "print(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "arcpy.env.workspace = \"C:\\\\Users\\\\Jayant\\\\Desktop\\\\SSTD\\\\workspace\"\n",
    "print(arcpy.env.workspace)\n",
    "t1 = datetime.now()\n",
    "arcpy.SpatialJoin_analysis('tabblock_20_filter.shp', # target_features\n",
    "                           'bg_20_filter.shp', # join_features \n",
    "                           '2020_hierarchy.shp', # out_feature_class\n",
    "                           'JOIN_ONE_TO_ONE', # join_operation\n",
    "                           'KEEP_ALL', # join_type\n",
    "                           None, # field_mapping\n",
    "                           'WITHIN', # match_option\n",
    "                           None, # search_radius\n",
    "                           None # distance_field_name\n",
    "                           )\n",
    "t2 = datetime.now()\n",
    "t2_delta = t2-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [field.name for field in arcpy.ListFields('hierarchy_2020.shp')]\n",
    "print(fields)\n",
    "\n",
    "print(arcpy.GetCount_management('hierarchy_2020.shp'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arcpy.env.workspace = \"C:\\\\Users\\\\Jayant\\\\Desktop\\\\SSTD\\\\test_workspace\"\n",
    "print(arcpy.env.workspace)\n",
    "print(arcpy.GetCount_management('tl_2020_27001_bg20.shp'))\n",
    "print(arcpy.GetCount_management('tl_2020_27001_tabblock20.shp'))\n",
    "arcpy.CopyFeatures_management('tl_2020_27001_bg20.shp','bg_20_filter.shp')\n",
    "arcpy.CopyFeatures_management('tl_2020_27001_tabblock20.shp','tablock_20_filter.shp')\n",
    "dropFields = ['NAMELSAD20', 'MTFCC20', 'FUNCSTAT20', 'ALAND20', 'AWATER20', 'INTPTLAT20', 'INTPTLON20', 'GEOID20']\n",
    "arcpy.DeleteField_management('bg_20_filter.shp', dropFields)\n",
    "dropFields = ['MTFCC20', 'UR20', 'UACE20', 'UATYPE20', 'FUNCSTAT20', 'ALAND20', 'AWATER20', 'INTPTLAT20', \n",
    "              'INTPTLON20', 'STATEFP20', 'COUNTYFP20', 'TRACTCE20', 'GEOID20', 'NAME20']\n",
    "arcpy.DeleteField_management('tablock_20_filter.shp', dropFields)\n",
    "t1 = datetime.now()\n",
    "arcpy.SpatialJoin_analysis('tablock_20_filter.shp', # target_features\n",
    "                           'bg_20_filter.shp', # join_features \n",
    "                           '2020_small_hierarchy.shp', # out_feature_class, 'STATEFP20', 'COUNTYFP20', 'TRACTCE20'\n",
    "                           'JOIN_ONE_TO_ONE', # join_operation\n",
    "                           'KEEP_ALL', # join_type\n",
    "                           None, # field_mapping\n",
    "                           'WITHIN', # match_option\n",
    "                           None, # search_radius\n",
    "                           None # distance_field_name\n",
    "                           )\n",
    "t2 = datetime.now()\n",
    "t2_delta = t2-t1\n",
    "print(\"Join completed in {0} seconds ({1} minutes)\".format(t2_delta.total_seconds(), t2_delta.total_seconds()/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arcpy.env.workspace = \"C:\\\\Users\\\\Jayant\\\\Desktop\\\\SSTD\\\\workspace\"\n",
    "print(arcpy.env.workspace)\n",
    "# Create income means by block, block group, tract, county\n",
    "t1 = datetime.now()\n",
    "arcpy.SpatialJoin_analysis('tl_2020_27001_tabblock10.shp', # target_features\n",
    "                           'rti_income_feature_class.shp', # join_features \n",
    "                           'rti_income_by_block_groups_2010_27001.shp', # out_feature_class\n",
    "                           'JOIN_ONE_TO_MANY', # join_operation\n",
    "                           'KEEP_ALL', # join_type\n",
    "                           None, # field_mapping\n",
    "                           'CONTAINS', # match_option\n",
    "                           None, # search_radius\n",
    "                           None # distance_field_name\n",
    "                           )\n",
    "t2 = datetime.now()\n",
    "t2_delta = t2-t1\n",
    "print(\"Assigned income points to block groups in {0} seconds ({1} minutes)\".format(t2_delta.total_seconds(), t2_delta.total_seconds()/60))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export grouped feature class to table for Gini calculation\n",
    "in_feature_class = \"rti_joined_rih_feature_class.shp\"\n",
    "out_location = 'C:\\\\Users\\\\Jayant\\\\Desktop\\\\SSTD\\\\outputs\\\\'\n",
    "out_filename = \"rti_joined_race_income_hhsize_features.csv\"\n",
    "\n",
    "outtable = arcpy.TableToTable_conversion(in_rows = in_feature_class, \n",
    "                     out_path = out_location, \n",
    "                     out_name = out_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_filename = 'C:\\\\Users\\\\alex\\\\Documents\\\\code\\\\maup_income_inequality\\\\maup_inequality\\\\outputs\\\\rti_income_grouped_by_block.csv'\n",
    "def getCountyIoD(in_filename, level_of_aggregation):\n",
    "    incomelist = pandas.read_csv(filepath_or_buffer=in_filename,\n",
    "                                 header=0,\n",
    "                                 index_col=False,\n",
    "                                 usecols=['hh_income', 'GEOID'],\n",
    "                                 engine='c')\n",
    "\n",
    "    # drop rows with NA - if there's no data for a census block, we will ignore it\n",
    "    # incomelist = incomelist.dropna()\n",
    "\n",
    "    # calculate mean income of each census block\n",
    "    # mean_income = incomelist.groupby('GEOID').mean()\n",
    "    # mean_income.hh_income = mean_income.hh_income.astype(numpy.longlong)\n",
    "    # mean_incomes = mean_income['hh_income']\n",
    "    # print(mean_income.head)\n",
    "\n",
    "    # print(\"Read in file. Calculating Gini...\")\n",
    "    # g = gini(mean_incomes.to_numpy())\n",
    "    # print(\"Gini is {0}\".format(g))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gini_From_Shape_File(input_shape_feature, shape_group_name, input_shape_statefips_name, input_shape_countyfips_name):\n",
    "    ## Filter shape file down to just Minnesota for speed\n",
    "    print(\"Creating subset of shape file just for Minnesota data...\")\n",
    "    t1 = datetime.now()\n",
    "    # Select only census blocks in MN\n",
    "    arcpy.SelectLayerByAttribute_management(input_shape_feature, \n",
    "                                            'NEW_SELECTION', \n",
    "                                            input_shape_statefips_name+\" = '27'\")\n",
    "\n",
    "    # Write the selected features to a new featureclass\n",
    "    arcpy.CopyFeatures_management(input_shape_feature, shape_group_name+\"_mn\")\n",
    "    t2 = datetime.now()\n",
    "    t2_delta = t2-t1\n",
    "    print(\"Created Subset of {0} in Minnesota only in {1} seconds\".format(\n",
    "        shape_group_name,\n",
    "        t2_delta.total_seconds()))\n",
    "    \n",
    "    print(\"Assigning income points to groups...\")\n",
    "    ## Create income means by block, block group, tract, county\n",
    "    t1 = datetime.now()\n",
    "    arcpy.SpatialJoin_analysis(shape_group_name+\"_mn\", # target_features\n",
    "                               'rti_income_feature_class', # join_features \n",
    "                               'rti_income_by_'+shape_group_name, # out_feature_class\n",
    "                               'JOIN_ONE_TO_MANY', # join_operation\n",
    "                               'KEEP_ALL', # join_type\n",
    "                               None, # field_mapping\n",
    "                               'CONTAINS', # match_option\n",
    "                               None, # search_radius\n",
    "                               None # distance_field_name\n",
    "                               )\n",
    "    t2 = datetime.now()\n",
    "    t2_delta = t2-t1\n",
    "    print(\"Assigned income points to {0} in {1} seconds ({2} minutes)\".format(\n",
    "        shape_group_name,\n",
    "        t2_delta.total_seconds(), \n",
    "        t2_delta.total_seconds()/60))\n",
    "    \n",
    "    print(\"Saving feature layer and reading in as raster...\")\n",
    "    ## export grouped feature class to table for Gini calculation\n",
    "    out_location = 'C:\\\\Users\\\\alex\\\\Documents\\\\code\\\\maup_income_inequality\\\\maup_inequality\\\\outputs\\\\'\n",
    "    out_filename = \"rti_income_grouped_by_\" + shape_group_name + \".csv\"\n",
    "    outtable = arcpy.TableToTable_conversion(\n",
    "                        in_rows = 'rti_income_by_'+shape_group_name, \n",
    "                        out_path = out_location, \n",
    "                        out_name = out_filename)\n",
    "\n",
    "    ## Calculate GINI\n",
    "    incomelist = pandas.read_csv(filepath_or_buffer=out_location+out_filename,\n",
    "                                 header=0,\n",
    "                                 index_col=False,\n",
    "                                 usecols=['hh_income', input_shape_countyfips_name],\n",
    "                                 engine='c')\n",
    "\n",
    "    # drop rows with NA - if there's no data for a census block, we will ignore it\n",
    "    incomelist = incomelist.dropna()\n",
    "\n",
    "    print(\"Calcuating Gini...\")\n",
    "    # calculate mean income of each census block\n",
    "    mean_income = incomelist.groupby(input_shape_countyfips_name).mean()\n",
    "    mean_income.hh_income = mean_income[input_shape_countyfips_name].astype(numpy.longlong)\n",
    "    #mean_incomes = mean_income['hh_income']\n",
    "    #print(mean_income.head)\n",
    "\n",
    "    g = gini(mean_incomes.to_numpy())\n",
    "    print(\"Gini is {0}\".format(g))\n",
    "    return\n",
    "    \n",
    "    \n",
    "    \n",
    "Gini_From_Shape_File('USA Block Groups', 'census_block_group', 'STATE_FIPS', 'STCOFIPS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape_feature = 'USA Block Groups'\n",
    "shape_group_name = 'census_block_group'\n",
    "input_shape_statefips_name = 'STATE_FIPS'\n",
    "input_shape_countyfips_name = 'STCOFIPS'\n",
    "out_location = 'C:\\\\Users\\\\alex\\\\Documents\\\\code\\\\maup_income_inequality\\\\maup_inequality\\\\outputs\\\\'\n",
    "out_filename = \"rti_income_grouped_by_\" + shape_group_name + \".csv\"\n",
    "\n",
    "## Calculate GINI\n",
    "incomelist = pandas.read_csv(filepath_or_buffer=out_location+out_filename,\n",
    "                             header=0,\n",
    "                             index_col=False,\n",
    "                             usecols=['hh_income', input_shape_countyfips_name],\n",
    "                             engine='c')\n",
    "\n",
    "# drop rows with NA - if there's no data for a census block, we will ignore it\n",
    "incomelist = incomelist.dropna()\n",
    "\n",
    "print(\"Calcuating Gini...\")\n",
    "# calculate mean income of each census block\n",
    "mean_income = incomelist.groupby(input_shape_countyfips_name).mean()\n",
    "mean_income.hh_income = mean_income.hh_income.astype(numpy.longlong)\n",
    "#mean_incomes = mean_income['hh_income']\n",
    "\n",
    "g = gini(mean_incomes.to_numpy())\n",
    "print(\"Gini is {0}\".format(g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_filename = 'C:\\\\Users\\\\alex\\\\Documents\\\\code\\\\maup_income_inequality\\\\maup_inequality\\\\outputs\\\\rti_income_grouped_by_block.csv'\n",
    "\n",
    "incomelist = pandas.read_csv(filepath_or_buffer=in_filename,\n",
    "                             header=0,\n",
    "                             index_col=False,\n",
    "                             usecols=['hh_income', 'GEOID', 'COUNTY', 'TRACT', 'BLKGRP', 'BLOCK'],\n",
    "                             engine='c')\n",
    "\n",
    "# drop rows with NA - if there's no data for a census block, we will ignore it\n",
    "incomelist = incomelist.dropna()\n",
    "\n",
    "# calculate mean income of each census block\n",
    "mean_income = incomelist.groupby(['COUNTY', 'TRACT', 'BLKGRP', 'BLOCK']).mean()\n",
    "mean_income.hh_income = mean_income.hh_income.astype(numpy.longlong)\n",
    "mean_incomes = mean_income['hh_income']\n",
    "#print(mean_income.head)\n",
    "g = gini(mean_incomes.to_numpy())\n",
    "print(\"Block Gini is {0}\".format(g))\n",
    "\n",
    "\n",
    "# calculate mean income of each census block group\n",
    "mean_income = incomelist.groupby(['COUNTY', 'TRACT', 'BLKGRP']).mean()\n",
    "mean_income.hh_income = mean_income.hh_income.astype(numpy.longlong)\n",
    "mean_incomes = mean_income['hh_income']\n",
    "g = gini(mean_incomes.to_numpy())\n",
    "print(\"Block Group Gini is {0}\".format(g))\n",
    "\n",
    "# calculate mean income of each census tract\n",
    "mean_income = incomelist.groupby(['COUNTY', 'TRACT']).mean()\n",
    "mean_income.hh_income = mean_income.hh_income.astype(numpy.longlong)\n",
    "mean_incomes = mean_income['hh_income']\n",
    "g = gini(mean_incomes.to_numpy())\n",
    "print(\"Tract Gini is {0}\".format(g))\n",
    "\n",
    "# calculate mean income of each county\n",
    "mean_income = incomelist.groupby(['COUNTY']).mean()\n",
    "mean_income.hh_income = mean_income.hh_income.astype(numpy.longlong)\n",
    "mean_incomes = mean_income['hh_income']\n",
    "g = gini(mean_incomes.to_numpy())\n",
    "print(\"County Gini is {0}\".format(g))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
